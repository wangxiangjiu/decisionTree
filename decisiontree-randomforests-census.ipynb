{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "import sys\n",
    "from math import ceil, sqrt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "TRAINING_DATA = \"census_data/train_data.csv\"\n",
    "TEST_DATA = \"census_data/test_data.csv\"\n",
    "UNKNOWN_VALUE_CONST = -1\n",
    "\n",
    "categorical_variable_indexes = [1, 3, 5, 6, 7, 8, 9, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1 = income >= 50k, 0 = income < 50k\n",
    "def load_data():\n",
    "    # process training data\n",
    "    f = open(TRAINING_DATA, \"r\")\n",
    "    features = np.array(f.readline()[:-1].split(',')[:-1])\n",
    "    data = []\n",
    "    for (i, line) in enumerate(f):\n",
    "        line = line[:-1]\n",
    "        data.append(np.array(line.split(',')))\n",
    "    data = np.array(data)\n",
    "    training_data, training_labels = data[:,:-1], data[:,-1].astype(int)\n",
    "    f.close()\n",
    "    \n",
    "    # process test data\n",
    "    f = open(TEST_DATA, \"r\")\n",
    "    f.readline()\n",
    "    test_data = []\n",
    "    for i,line in enumerate(f):\n",
    "        line = line[:-1]\n",
    "        test_data.append(np.array(line.split(',')))\n",
    "    test_data = np.array(test_data)\n",
    "    return features, training_data, training_labels, test_data\n",
    "\n",
    "features, pre_training_data, training_labels, pre_test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_data(pre_training_data, pre_test_data):\n",
    "    training_data, test_data = pre_training_data, pre_test_data\n",
    "    for category_index in categorical_variable_indexes:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(np.concatenate((training_data[:,category_index], test_data[:,category_index])))\n",
    "        training_data[:,category_index] = le.transform(training_data[:,category_index])\n",
    "        test_data[:,category_index] = le.transform(test_data[:,category_index])\n",
    "        # replace unknown value (?) with \"-1\"\n",
    "        if \"?\" in le.classes_:\n",
    "            unknown_index = le.transform([\"?\"])[0]\n",
    "            column = training_data[:,category_index].astype(int)\n",
    "            training_data[:,category_index][column == unknown_index] = UNKNOWN_VALUE_CONST\n",
    "            column = test_data[:,category_index].astype(int)\n",
    "            test_data[:,category_index][column == unknown_index] = UNKNOWN_VALUE_CONST\n",
    "        \n",
    "    return training_data, test_data\n",
    "\n",
    "training_data, test_data = preprocess_data(pre_training_data, pre_test_data)\n",
    "training_data = training_data.astype(int)\n",
    "test_data = test_data.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def entropy(data):\n",
    "    if data is None or len(data) == 0:\n",
    "        return 0.0\n",
    "    counter = defaultdict(lambda: 0)\n",
    "    for item in data:\n",
    "        counter[item] += 1\n",
    "    en, num_items = 0.0, len(data)\n",
    "    for key, value in counter.items():\n",
    "        p = value / num_items\n",
    "        en = en - p * np.log2(p)\n",
    "    return en\n",
    "\n",
    "assert entropy([1, 1, 2, 3]) == 1.5\n",
    "assert abs(entropy([100, 100, 125, 150, 150]) - 1.5219280948873622) < 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def two_way_splits(length):\n",
    "    splits_list = []\n",
    "    if length <= 21:\n",
    "        for i in range(1, 1 << (length - 1)):\n",
    "            current, other = [], []\n",
    "            for j in range(length):\n",
    "                if (i >> j) & 1 == 1:\n",
    "                    current.append(j)\n",
    "                else:\n",
    "                    other.append(j)\n",
    "            splits_list.append([current, other])\n",
    "        if len(splits_list) > MAX_NUM_FEATURES:\n",
    "            splits_list = np.array(splits_list)\n",
    "            random_indexes = np.random.choice(len(splits_list), MAX_NUM_FEATURES, replace=False)\n",
    "            splits_list = splits_list[random_indexes]\n",
    "    else:\n",
    "        for iteration in range(MAX_NUM_FEATURES):\n",
    "            num_left_subset = random.randint(1, length - 1)\n",
    "            num_right_subset = length - num_left_subset\n",
    "            left_indexes = np.random.choice(length, num_left_subset, replace=False)\n",
    "            right_indexes = np.delete(np.arange(length), left_indexes)\n",
    "            splits_list.append([left_indexes, right_indexes])\n",
    "    return splits_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, left, right, split_feature, is_leaf_node):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.split_feature = split_feature\n",
    "        self.is_leaf_node = is_leaf_node\n",
    "        \n",
    "class QuantitativeNode(Node):\n",
    "    def __init__(self, left, right, split_feature, split_threshold):\n",
    "        super(QuantitativeNode, self).__init__(left, right, split_feature, False)\n",
    "        self.is_categorical_node = False\n",
    "        self.split_threshold = split_threshold\n",
    "    \n",
    "class CategoricalNode(Node):\n",
    "    def __init__(self, left, right, split_feature, left_subset_vals, right_subset_vals):\n",
    "        super(CategoricalNode, self).__init__(left, right, split_feature, False)\n",
    "        self.is_categorical_node = True\n",
    "        # values that go into left, right nodes\n",
    "        self.left_subset_vals = left_subset_vals\n",
    "        self.right_subset_vals = right_subset_vals\n",
    "\n",
    "class LeafNode(Node):\n",
    "    def __init__(self, label):\n",
    "        super(LeafNode, self).__init__(None, None, None, True)\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is leaf node False\n",
      "split feature, value fnlwgt 2\n",
      "split threshold 2\n",
      "is leaf node True\n",
      "is leaf node False\n",
      "split feature, value fnlwgt 3\n",
      "split threshold 2\n",
      "is leaf node False\n",
      "split feature, value age 2\n",
      "subset vals [1] [2]\n",
      "is leaf node True\n",
      "is leaf node False\n",
      "split feature, value fnlwgt 2\n",
      "split threshold 2\n",
      "is leaf node True\n",
      "is leaf node False\n",
      "split feature, value fnlwgt 5\n",
      "split threshold 2\n",
      "is leaf node False\n",
      "split feature, value age 1\n",
      "subset vals [1] [2]\n",
      "is leaf node False\n",
      "split feature, value workclass 125\n",
      "split threshold 125\n",
      "is leaf node True\n",
      "is leaf node False\n",
      "split feature, value fnlwgt 4\n",
      "split threshold 2\n",
      "is leaf node False\n",
      "split feature, value age 1\n",
      "subset vals [1] [2]\n",
      "is leaf node False\n",
      "split feature, value workclass 150\n",
      "split threshold 125\n",
      "is leaf node True\n"
     ]
    }
   ],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, category_vars, attr_bagging=False, use_limit_heuristics=False):\n",
    "        self.category_vars = category_vars\n",
    "        self.attr_bagging = attr_bagging\n",
    "        self.use_limit_heuristics = use_limit_heuristics\n",
    "        \n",
    "    def build_tree(self, data_indexes, training_data, training_labels, current_height):\n",
    "        assert data_indexes.shape[0] > 0\n",
    "        \n",
    "        cur_training_data = training_data[data_indexes]\n",
    "        cur_training_labels = training_labels[data_indexes]\n",
    "        mode = stats.mode(cur_training_labels, axis=None)\n",
    "        mode_percentage = mode[1][0] / cur_training_labels.shape[0]\n",
    "        usable_features = np.array([index for index in range(self.M) if np.unique(cur_training_data[:,index]).shape[0] > 1])\n",
    "\n",
    "        if np.unique(cur_training_labels).shape[0] == 1 or \\\n",
    "            usable_features.shape[0] == 0 or \\\n",
    "            (self.use_limit_heuristics and (current_height == HEIGHT_STOP_THRESHOLD or \\\n",
    "            mode_percentage >= MODE_LABEL_STOP_THRESHOLD or data_indexes.shape[0] <= VOLUME_STOP_THRESHOLD)):\n",
    "            return LeafNode(mode[0][0])\n",
    "        \n",
    "        if self.attr_bagging:\n",
    "            feature_indexes = np.random.choice(len(usable_features), ceil(sqrt(len(usable_features))), replace=False)\n",
    "        else:\n",
    "            feature_indexes = np.arange(0, len(usable_features))\n",
    "        feature_indexes = usable_features[np.array(feature_indexes, dtype=np.int)]\n",
    "        split_data = self.segmenter(cur_training_data, cur_training_labels, feature_indexes, \\\n",
    "                                    data_indexes, self.information_gain, np.mean)\n",
    "        \n",
    "        is_categorical_feature, best_feature = split_data[0], split_data[1]\n",
    "        if is_categorical_feature:\n",
    "            left_subset_vals, right_subset_vals = split_data[2]\n",
    "        else:\n",
    "            split_threshold = split_data[2]\n",
    "            \n",
    "        best_left_indexes, best_right_indexes = split_data[3], split_data[4]\n",
    "        \n",
    "        left_node = self.build_tree(best_left_indexes, training_data, training_labels, current_height + 1)\n",
    "        right_node = self.build_tree(best_right_indexes, training_data, training_labels, current_height + 1)\n",
    "        if is_categorical_feature:\n",
    "            current_node = CategoricalNode(left_node, right_node, best_feature, \\\n",
    "                                           left_subset_vals, right_subset_vals)\n",
    "        else:\n",
    "            current_node = QuantitativeNode(left_node, right_node, best_feature, split_threshold)\n",
    "        return current_node\n",
    "    \n",
    "    def train(self, training_data, training_labels):        \n",
    "        self.N = training_data.shape[0]\n",
    "        self.M = training_data.shape[1]\n",
    "        \n",
    "        data_indexes = np.arange(0, training_data.shape[0])\n",
    "        self.root = self.build_tree(data_indexes, training_data, training_labels, 1)\n",
    "        \n",
    "    def predict_recursive(self, current_node, data_item):\n",
    "        if current_node.is_leaf_node:\n",
    "            return current_node.label\n",
    "\n",
    "        feature_value = data_item[current_node.split_feature]\n",
    "        if current_node.is_categorical_node:\n",
    "            if feature_value in current_node.left_subset_vals:\n",
    "                return self.predict_recursive(current_node.left, data_item)\n",
    "            else:\n",
    "                return self.predict_recursive(current_node.right, data_item)\n",
    "        else:\n",
    "            if feature_value <= current_node.split_threshold:\n",
    "                return self.predict_recursive(current_node.left, data_item)\n",
    "            else:\n",
    "                return self.predict_recursive(current_node.right, data_item)\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        predictions = []\n",
    "        for data_item in test_data:\n",
    "            predictions.append(self.predict_recursive(self.root, data_item))\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def information_gain(self, left_data, right_data):\n",
    "        \n",
    "        parent_data = np.hstack((left_data, right_data))\n",
    "        left_len = len(left_data)\n",
    "        right_len = len(right_data)\n",
    "        total_len = left_len + right_len\n",
    "        return entropy(parent_data) - \\\n",
    "               (left_len / total_len) * entropy(left_data) - \\\n",
    "               (right_len / total_len) * entropy(right_data)\n",
    "    \n",
    "    def impurity(self, left_label_hist, right_label_hist):\n",
    "        \"\"\"\n",
    "        A method that takes in the result of a split: two histograms\n",
    "        (a histogram is a mapping from label values to their frequencies)\n",
    "        that count the frequencies of labels on the ”left” and ”right”\n",
    "        side of that split. The method calculates and outputs a scalar\n",
    "        value representing the impurity (i.e. the ”badness”) of the specified\n",
    "        split on the input data.\n",
    "        \"\"\"\n",
    "        total_count = sum(left_label_hist.values())\n",
    "        val = 0.0\n",
    "        for key, value in left_label_hist.items():\n",
    "            val += (value * value / total_count)\n",
    "        for key, value in right_label_hist.items():\n",
    "            val += (value * value / total_count)    \n",
    "        return -val\n",
    "    \n",
    "    def segmenter(self, data, labels, features_used, indexes, metric_func, unknown_predictor_func):\n",
    "        \"\"\"\n",
    "        A method that takes in data and labels. When called, it finds the \n",
    "        best split rule for a Node using the impurity measure and input data.\n",
    "        There are many different types of segmenters you might implement,\n",
    "        each with a different method of choosing a threshold. The usual method\n",
    "        is exhaustively trying lots of different threshold values from the data\n",
    "        and choosing the combination of split feature and threshold with the\n",
    "        lowest impurity value. The final split rule uses the split feature with\n",
    "        the lowest impurity value and the threshold chosen by the segmenter.\n",
    "        \"\"\"\n",
    "        max_metric, best_feature, best_left_indexes, best_right_indexes = \\\n",
    "            -1e100, None, None, None\n",
    "        left_subset_vals, right_subset_vals, split_threshold = None, None, None\n",
    "\n",
    "        for feature_number in features_used:\n",
    "            feature_list = data[:,feature_number]\n",
    "            feature_list[feature_list == UNKNOWN_VALUE_CONST] = unknown_predictor_func(feature_list)\n",
    "            \n",
    "            unique_sorted_features = np.sort(np.unique(feature_list))\n",
    "            if unique_sorted_features.shape[0] >= MAX_NUM_FEATURES:\n",
    "                unique_sorted_features = unique_sorted_features[::unique_sorted_features.shape[0]//MAX_NUM_FEATURES]\n",
    "            \n",
    "            feature_list_with_indexes = np.dstack((feature_list, np.arange(feature_list.shape[0])))[0]\n",
    "            feature_list_with_indexes = feature_list_with_indexes[feature_list_with_indexes[:,0].argsort()]\n",
    "\n",
    "            if feature_number in self.category_vars:\n",
    "                # categorical variables\n",
    "                subset_splits = two_way_splits(unique_sorted_features.shape[0])\n",
    "\n",
    "                for (i, (left_idxs, right_idxs)) in enumerate(subset_splits):\n",
    "                    left_vals = unique_sorted_features[left_idxs]\n",
    "                    right_vals = unique_sorted_features[right_idxs]\n",
    "                    \n",
    "                    left_indexes = np.where(np.in1d(feature_list, left_vals))[0]\n",
    "                    right_indexes = np.where(np.in1d(feature_list, right_vals))[0]\n",
    "                    info_gain = metric_func(labels[left_indexes], labels[right_indexes])\n",
    "                    \n",
    "                    if info_gain > max_metric or \\\n",
    "                        (info_gain == max_metric and \\\n",
    "                         abs(len(left_indexes) - len(right_indexes)) < abs(len(best_left_indexes) - len(best_right_indexes.shape))):\n",
    "                        left_subset_vals = left_vals\n",
    "                        right_subset_vals = right_vals\n",
    "                        \n",
    "                        max_metric = info_gain\n",
    "                        best_left_indexes, best_right_indexes = indexes[left_indexes], indexes[right_indexes]\n",
    "                        best_feature = feature_number\n",
    "            else:\n",
    "                # quantitative variables\n",
    "                left_indexes, right_indexes = [], feature_list_with_indexes[:,1][::-1].tolist()\n",
    "                pointer_index = 0\n",
    "                for split_value in unique_sorted_features[:-1]:\n",
    "                    while feature_list_with_indexes[pointer_index][0] <= split_value:\n",
    "                        left_indexes.append(feature_list_with_indexes[pointer_index][1])\n",
    "                        right_indexes.pop()\n",
    "                        pointer_index += 1\n",
    "                    \n",
    "                    left_indexes_np = np.array(left_indexes, dtype=np.int)\n",
    "                    right_indexes_np = np.array(right_indexes, dtype=np.int)\n",
    "                    info_gain = metric_func(labels[left_indexes_np], labels[right_indexes_np])                    \n",
    "                    \n",
    "                    if info_gain > max_metric or \\\n",
    "                        (info_gain == max_metric and \\\n",
    "                         abs(len(left_indexes) - len(right_indexes)) < abs(len(best_left_indexes) - len(best_right_indexes.shape))):\n",
    "                        split_threshold = split_value\n",
    "                        \n",
    "                        max_metric = info_gain\n",
    "                        best_left_indexes, best_right_indexes = indexes[left_indexes_np], indexes[right_indexes_np]\n",
    "                        best_feature = feature_number\n",
    "                        \n",
    "        is_categorical_feature = best_feature in self.category_vars\n",
    "        if is_categorical_feature:\n",
    "            return (is_categorical_feature, best_feature, (left_subset_vals, right_subset_vals), \\\n",
    "                    best_left_indexes, best_right_indexes)\n",
    "        else:\n",
    "            return (is_categorical_feature, best_feature, split_threshold, \\\n",
    "                    best_left_indexes, best_right_indexes)\n",
    "        \n",
    "classifier = DecisionTree([0], use_limit_heuristics=False)\n",
    "X = np.array([ \\\n",
    "    [1, 100, 2],\n",
    "    [2, 150, 3],\n",
    "    [2, 125, 2],\n",
    "    [1, 125, 5],\n",
    "    [1, 150, 4]\n",
    "])\n",
    "y = np.array([1, 0, 1, 0, 1])\n",
    "classifier.train(X, y)\n",
    "assert np.array_equal(classifier.predict(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MODE_LABEL_STOP_THRESHOLD = 0.98\n",
    "HEIGHT_STOP_THRESHOLD = 15\n",
    "VOLUME_STOP_THRESHOLD = 10\n",
    "MAX_NUM_FEATURES = 1024\n",
    "tree_classifier = DecisionTree(categorical_variable_indexes, use_limit_heuristics=True)\n",
    "tree_classifier.train(training_data, training_labels)\n",
    "tree_prediction = tree_classifier.predict(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.839933993399\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(prediction == training_labels) / training_data.shape[0])\n",
    "tree_test_prediction = tree_classifier.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"decision_tree_census.csv\", \"w\") as f:\n",
    "    f.write(\"Id,Category\\n\")\n",
    "    for i, val in enumerate(tree_test_prediction):\n",
    "        f.write(\"{},{}\\n\".format(i+1, val))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating tree 0 of 50..\n",
      "Generating tree 5 of 50..\n",
      "Generating tree 10 of 50..\n",
      "Generating tree 15 of 50..\n",
      "Generating tree 20 of 50..\n",
      "Generating tree 25 of 50..\n",
      "Generating tree 30 of 50..\n",
      "Generating tree 35 of 50..\n",
      "Generating tree 40 of 50..\n",
      "Generating tree 45 of 50..\n"
     ]
    }
   ],
   "source": [
    "class RandomForest(DecisionTree):\n",
    "    def __init__(self, category_vars, num_trees, attr_bagging=True, use_limit_heuristics=True):\n",
    "        super(RandomForest, self).__init__(category_vars, attr_bagging, use_limit_heuristics)\n",
    "        self.num_trees = num_trees\n",
    "        \n",
    "    def generate_random_tree(self):\n",
    "        # data bagging\n",
    "        bag_indexes = np.random.choice(self.N, self.N, replace=True)\n",
    "        training_data_bag, training_labels_bag = \\\n",
    "            self.training_data[bag_indexes], self.training_labels[bag_indexes]\n",
    "        data_indexes = np.arange(0, self.N)\n",
    "\n",
    "        return self.build_tree(data_indexes, training_data_bag, training_labels_bag, 1)\n",
    "        \n",
    "    def train(self, training_data, training_labels):\n",
    "        self.N = training_data.shape[0]\n",
    "        self.M = training_data.shape[1]\n",
    "        self.training_data = training_data\n",
    "        self.training_labels = training_labels\n",
    "        \n",
    "        data_indexes = np.arange(0, training_data.shape[0])\n",
    "        \n",
    "        self.roots = []\n",
    "        \n",
    "        for i in range(self.num_trees):\n",
    "            if i % (self.num_trees // 10) == 0:\n",
    "                print(\"Generating tree {} of {}..\".format(i, self.num_trees))\n",
    "                sys.stdout.flush()\n",
    "            self.roots.append(self.generate_random_tree())\n",
    "            \n",
    "    def predict(self, test_data):\n",
    "        predictions = []\n",
    "        for data_item in test_data:\n",
    "            forest_predictions = [self.predict_recursive(root, data_item) for root in self.roots]\n",
    "            mode_value = stats.mode(forest_predictions)[0][0]\n",
    "            predictions.append(mode_value)\n",
    "        return np.array(predictions)\n",
    "    \n",
    "classifier = RandomForest([0], 50, use_limit_heuristics=False)\n",
    "X = np.array([ \\\n",
    "    [1, 100, 2],\n",
    "    [2, 150, 3],\n",
    "    [1, 125, 1],\n",
    "    [2, 125, 5],\n",
    "    [1, 150, 4],\n",
    "    [2, 100, 3]\n",
    "])\n",
    "y = np.array([1, 0, 1, 0, 1, 1])\n",
    "classifier.train(X, y)\n",
    "assert np.array_equal(classifier.predict(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating tree 0 of 50..\n",
      "Generating tree 5 of 50..\n",
      "Generating tree 10 of 50..\n",
      "Generating tree 15 of 50..\n",
      "Generating tree 20 of 50..\n",
      "Generating tree 25 of 50..\n",
      "Generating tree 30 of 50..\n",
      "Generating tree 35 of 50..\n",
      "Generating tree 40 of 50..\n",
      "Generating tree 45 of 50..\n"
     ]
    }
   ],
   "source": [
    "MODE_LABEL_STOP_THRESHOLD = 0.90\n",
    "HEIGHT_STOP_THRESHOLD = 5\n",
    "VOLUME_STOP_THRESHOLD = 50\n",
    "MAX_NUM_FEATURES = 128\n",
    "random_forest_classifier = RandomForest(categorical_variable_indexes, 50, use_limit_heuristics=True)\n",
    "random_forest_classifier.train(training_data, training_labels)\n",
    "random_training_prediction = random_forest_classifier.predict(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.835869698081\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(random_training_prediction == training_labels) / training_labels.shape[0])\n",
    "random_test_prediction = random_forest_classifier.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"random_forest_census.csv\", \"w\") as f:\n",
    "    f.write(\"Id,Category\\n\")\n",
    "    for i, val in enumerate(random_test_prediction):\n",
    "        f.write(\"{},{}\\n\".format(i+1, val))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relationship 0\n",
      "[1 2 3 4] [0 5]\n",
      "education-num 5\n",
      "11\n",
      "capital-gain 0\n",
      "5013\n",
      "occupation 6\n",
      "[ 1  2  4 10 12 13] [ 3  5  6  7  8  9 11 14]\n",
      "education 6\n",
      "[ 0  1  2  3  4  5  6 13] [ 8 11 15]\n",
      "age 39\n",
      "66\n",
      "age 39\n",
      "37\n",
      "education-num 5\n",
      "3\n",
      "fnlwgt 347434\n",
      "36209\n",
      "capital-loss 0\n",
      "1740\n",
      "fnlwgt 347434\n",
      "64102\n",
      "native-country 26\n",
      "[12  6 14 38 28  3  7 17 27 11  8 31 13 32  1 35] [ 2  4  5 21 22 26 30 33 36 39 41]\n",
      "workclass 4\n",
      "[5] [1 2 3 4 6 7]\n",
      "hours-per-week 43\n",
      "45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_data_point = [training_data[1234]]\n",
    "tree_classifier.predict(random_data_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education-num 12 6\n",
      "capital-gain 6849 7\n",
      "relationship [array([1, 2, 3, 4]), array([0, 5])] 15\n",
      "hours-per-week 40 1\n",
      "marital-status [array([1, 2]), array([0, 3, 4, 5, 6])] 9\n",
      "occupation [array([ 1,  3,  5,  6,  7,  8, 13]), array([ 2,  4,  9, 10, 11, 12, 14])] 6\n",
      "age 28 5\n",
      "sex [array([0]), array([1])] 1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "cnt = defaultdict(lambda: 0)\n",
    "d = defaultdict(lambda: 0)\n",
    "for root in random_forest_classifier.roots:\n",
    "    if root.is_categorical_node:\n",
    "        d[features[root.split_feature]] = [root.left_subset_vals, root.right_subset_vals]\n",
    "        cnt[features[root.split_feature]] += 1\n",
    "    else:\n",
    "        d[features[root.split_feature]] = root.split_threshold\n",
    "        cnt[features[root.split_feature]] += 1\n",
    "for k, v in d.items():\n",
    "    print(k, v, cnt[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workclass ['?' 'Federal-gov' 'Local-gov' 'Never-worked' 'Private' 'Self-emp-inc'\n",
    " 'Self-emp-not-inc' 'State-gov' 'Without-pay']\n",
    "education ['10th' '11th' '12th' '1st-4th' '5th-6th' '7th-8th' '9th' 'Assoc-acdm'\n",
    " 'Assoc-voc' 'Bachelors' 'Doctorate' 'HS-grad' 'Masters' 'Preschool'\n",
    " 'Prof-school' 'Some-college']\n",
    "marital-status ['Divorced' 'Married-AF-spouse' 'Married-civ-spouse'\n",
    " 'Married-spouse-absent' 'Never-married' 'Separated' 'Widowed']\n",
    "occupation ['?' 'Adm-clerical' 'Armed-Forces' 'Craft-repair' 'Exec-managerial'\n",
    " 'Farming-fishing' 'Handlers-cleaners' 'Machine-op-inspct' 'Other-service'\n",
    " 'Priv-house-serv' 'Prof-specialty' 'Protective-serv' 'Sales'\n",
    " 'Tech-support' 'Transport-moving']\n",
    "relationship ['Husband' 'Not-in-family' 'Other-relative' 'Own-child' 'Unmarried' 'Wife']\n",
    "race ['Amer-Indian-Eskimo' 'Asian-Pac-Islander' 'Black' 'Other' 'White']\n",
    "sex ['Female' 'Male']\n",
    "native-country ['?' 'Cambodia' 'Canada' 'China' 'Columbia' 'Cuba' 'Dominican-Republic'\n",
    " 'Ecuador' 'El-Salvador' 'England' 'France' 'Germany' 'Greece' 'Guatemala'\n",
    " 'Haiti' 'Holand-Netherlands' 'Honduras' 'Hong' 'Hungary' 'India' 'Iran'\n",
    " 'Ireland' 'Italy' 'Jamaica' 'Japan' 'Laos' 'Mexico' 'Nicaragua'\n",
    " 'Outlying-US(Guam-USVI-etc)' 'Peru' 'Philippines' 'Poland' 'Portugal'\n",
    " 'Puerto-Rico' 'Scotland' 'South' 'Taiwan' 'Thailand' 'Trinadad&Tobago'\n",
    " 'United-States' 'Vietnam' 'Yugoslavia']"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
